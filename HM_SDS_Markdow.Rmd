---
title: "Homework 1 of Statistical of Data Science"
author: "Barba Paolo, Candi Matteo"
date: ''
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## About us:
This report is made by two students of Data Science during the accademic year 2022/2023 for the exam of Statistcs for Data Science teached by prof : Pierpaolo Brutti.

![Us](/Users/paolo/Desktop/Documents/GitHub/SDS_HW1/Picture/surf.jpeg)



# Exercise 1 Stopping Time

### Process:
Suppose that $X ∼ Unif(0, 1)$, and suppose we independently draw ${Y_{1}, Y_{2}, Y_{3}, \dots}$ from yet another $Unif(0, 1)$ model until we reach the random stopping time T such that $(Y_{T} < X)$.

### Simulation study:
The idea is the following :
We generate m Unif(0,1) and then use them as a parameters of the geometric distributions.
$X ∼ Unif(0, 1)$ and the distribution of $Y | X = x$ is a geometric distribution .

We run  a simulation with size:
M = { 100, 1000, 10000, 100000, 1000000, 10000000}

```{r simualtion  time}
rm(list=ls())
set.seed(13112221)
Sim <- c(100, 1000, 10000, 100000, 1000000, 10000000)  # vector of the size 
fin <- c()    # inizialize the the vector of the result
timing_fun <- function(s){
  beg <- Sys.time()     # starting time
  stop_simulations <-  rgeom(n = s, prob = runif(n = s))   # run the simulation
  fin <- Sys.time() - beg #ending time
  return(fin)
}

results <- sapply(Sim , timing_fun)

tab_time <- data.frame (Simulation_size  = c("100", "1000", "10000", "100000", "1000000", "10000000") , Computational_time = results  )   # creating the dataframe of the results
tab_time
paste("The median speed over M is : " , median(results))

```

Now we can start the anyalisis with a fixed M size equal to 100000
```{r simualtion }
M <-  100000               # simulation size
beg <- Sys.time()         # start time
stop_simulation <-rgeom(n = M, prob = runif(n = M))     #runninng the simulation 
fin <- Sys.time() - beg   # end time
p_hat <- proportions( table( stop_simulation ))

```

## Comparison with the true model

First we compute the true function:

$$ Pr(T = t) = \frac{1}{t(t+1)}$$ for t $∈${1,2,3,...}


```{r true function }
pT <- function(t)  1/(t*(t+1))  
t_seq <- 1:25      #Plot it zooming between t = 1 and t = 25
```


```{r plots }

{plot(t_seq, pT(t_seq), 
     type = "h",
     lwd = 4,
     col = "cyan4",
     ylab = expression(p[T]),
     xlab = "t",
     main = "Marginal of the Stopping Time",
     sub = paste("Simulation size:", M))

points(t_seq, p_hat[t_seq], pch = 24, 
       col = "black", bg = "yellow", cex = .7 )

legend("topright", legend = c("True model" , "simulation result"),
       col = c("cyan4","yellow"),lty=c(1,NA),pch=c(NA,24), cex=0.7,
       box.lty=0)
grid()}
```


Now we can have a look at the series of the averages.
```{r Average }
step_size = 10    #set a step size
ave_step  = seq(1, length(stop_simulation), step_size)  #start the sequence
ave_vec = rep(NA, length(ave_step))                     #pre-allocating the vector

# Compute the simulations averages
t <- 0
for (i in ave_step){
  t <- t + 1
  ave_vec[t] = mean( stop_simulation[1:i] )
}


```

```{r average}
{plot(ave_vec , main = "Average stopping time",
     xlab = "step", ylab = "Averege stopping time",
     sub = "Simulation study", col = "steelblue" , type = "l" , lwd =3)
grid()}
```







## Computational Analysis:

In this section we would quantitatively check how the simulation size impacts the approximation, in order to show that, we are going to compute the distances between the true distribution and the estimated distibution.

```{r simualtion analysis , eval = FALSE}
for (j in 1:M){
  i <- 1
  for (s in Sim){    
    stop_simulations <-  rgeom(n = s, prob = runif(n = s))   # run the simulation
    p_hat <- proportions(table(stop_simulations))
    
    diff[i,j] <- sum(pT(t_seq) - p_hat[t_seq], na.rm = T) 
    i <- i + 1
}
}

```



```{r plots error, eval = FALSE}
plot(diff[1,] , ylim = c(-0.009,0.009), type = "l" , main = "Error") 
points(diff[2,] , col = 'blue' , type = "l")
points(diff[3,] , col = 'red', type = "l")  
points(diff[4,] , col = 'green', type = "l")
points(diff[5,] , col = "pink", type = "l")
```




![errors plot](/Users/paolo/Desktop/Documents/GitHub/SDS_HW1/Picture/error.png)


The results show an obvious reduction of the errors over M , the greatest difference can be seen from the passage from M = 100 to M = 1000

## Statistical Analysis

In this section we supposed to be a Casino that wants to introduce this experiment in the available games. The costumers have to pay an amount of C euros to start this game and he win as many euros as the waiting time.

The question we are interested to answer is: 
What is the "fair" amount in order to play this game?.

To answer the question, we need to compute the expected value of this random variable. How much you pay to enter the game is the expected value of the random variable.


On average you expect to win an infinite amount from this game, so according to traditional expected value theory, you can afford to pay any amount C euros to play.The mathematical result provides as a solution that it is convenient to pay any amount of euros to play this game.


In fact, repeating this experiment many time, it will have to happen (with very low probability) to win a lot, enough for paying all the expenses incurred before.

In practice though, no reasonable person is willing to pay a lot of money to play this game. The intuitive refusal to invest large sums in the game is well supported by the simulation described in the following graph. By repeating the series 10,000 times, on average low winnings are obtained at the beginning (a few units). Subsequently the average rises, corresponding to some lucky series, and then decreases slightly until the next lucky hit. The overall trend is undoubtedly growing, and will mathematically tend to infinity after an infinite series of plays but, in the 10,000 plays of the simulation, the average of the winnings has just reached the value of 10.

From the statistical point of view, no difficulties came from the situation presented in the game. In other words, it is perfectly coherent to accept the  possibility of an infinite win, such as to balance any amount paid in the (infinite) times the win is insignificant.


### Conclusion:
Ultimately, the decision to play or not to play this game  must depend on each individual's risk, itself dependent on many parameters, such as initial revenue or the amount you are willing to lose.

If the earnings here are "on average" infinite, you must also have infinite funds and play an infinite number of times to be eligible for certain earnings.

No casinos would introduce  this game, having the risk of infinite loss. A possible solution is to force the game to finish with a maximum win of L after the L-th trials.


### Comments and further analysis:

No casinos would introduce this game, having the risk of infinite loss. A possible solution is to force the game to finish with a maximum win of L after the L-th trials.

Considering the fact of the maximum revenue fixed to L = 8 in the simulation study it results that a "fair" amount to pay is :

```{r fixed amount}
L = 8    # max amount it can be won
stop_simulation_copy <- stop_simulation
stop_simulation_copy[stop_simulation_copy>7] <- L  # set all the observation greater than 7 to L
print(mean(stop_simulation_copy))
```

```{r atoher plot }
tab = proportions(table(stop_simulation_copy))
library(ggplot2)
# Create Data
data=data.frame(tab)
ggplot(data, aes(x=stop_simulation_copy, y=Freq,fill=stop_simulation_copy))+
  geom_bar(stat="identity") +
  theme_minimal() +
  geom_bar(stat="identity", fill="steelblue")+
  xlab("stop time") + ylab("Frequencies") +  guides(fill="none") +
  ggtitle("Barplot of stopping time" ) + 
  scale_y_continuous(labels=scales::percent)  

```

Remark:
50% of the time the stopping time is equal to 0 and no payoff for the player.
The percentege of the time when the game is stopped is
```{r percentage}

print(data$Freq[data$stop_simulation_copy == 8])
```
----
## Exercise 2 Differential Privacy

### 2.1 

In this section we focus on the univariate case with dimensionality $d = 1$ and we set up a simulation in order to compute the MISE: Mean Integreated Squared Error between the true model $p_{X}(·)$ and the two approximation $\hat{p}_{n,m}(·)$ and $\hat{q}_{\epsilon,m}(·)$ where:
$$\hat{p}_{n,m}(·) = \sum_{j=1}^{m} \frac{\hat{p}_{j}}{h^{d}} \mathbb{1}( x \in B_{j}) $$
$$\hat{q}_{\epsilon,m}(·) = \sum_{j=1}^{m} \frac{\hat{q}_{j}}{h^{d}} \mathbb{1}( x \in B_{j}) $$
```{r packages}
rm(list=ls())       #Clear output
library(VGAM)       # Library for the Laplacian Function
set.seed(13112221)  # For reprocibility

```
We compute the step function for both \hat{p} and \hat{q}
```{r Step Function for p_hat}

##### Step Function for q_hat #####

# Defining step functions for p_hat 
p_hat_func <- function(x , bins , p_hat ){
  interval <- cut(x, bins, include.lowest = T)
  levels   <- levels(interval)
  f        <- p_hat[interval == levels]
  return(f)
}

##### Step Function for q_hat #######
q_hat_func <- function(x, bins, q_hat){
  interval <- cut(x, bins, include.lowest = T)
  levels   <- levels(interval)
  f        <- q_hat[interval == levels]
  return(f)
}
```

Here we wrote the simulation function we are going to run
```{r simulation function}
#####  Simulation Function #######


simulation_function <- function(m , sim_size = 100, n=100, h = 1/m , eps = .1, func='beta'){
  print(m)
  
  if(func == 'beta'){
    distr = function(x) dbeta(x,shape1 = 10, shape2 = 10)
    sample_distr = function(n) rbeta(n, 10, 10)} 
  else if(func == 'mixture'){
    distr = dmixture
    sample_distr = function(n) rmixture(n)}
  else{stop("The 'func' input is wrong. Choose between 'beta' or 'mixture'!")}
  
  for(rep in 1:sim_size){  
    
    integral_p <- c()       # Pre-allocate the vector of the integral
    integral_q <- c()       # Pre-allocate the vector of the integral
    
    X <- sample_distr(n)   # Generating the random sample from the beta
    
    bins <- seq(0, 1, h)    # Set the bins
    
    intervals <- cut(X, bins, include.lowest = T)  # Rename units with the bins they belong
    
    
    pj_hat <- table(intervals) / n              # Finding the frequencies of units inside each bins
    
    
    p_hat <- as.vector(pj_hat / h)              # Computing high of each bin dividing the frequencies for the width of the bin
    
    nu <- rlaplace(m, 0, 2/eps)                 # Generating m values from a Laplacian: one for each bin
    
    
    Dj <- table(intervals) + nu                 # Adding nu to every absolute frequencies of each bin
    
    Dj[Dj < 0] = 0    # Set all the nagative values to 0 t                      
    qj_hat = Dj
    
    # Finding qj_hat dividing max(0, Dj) for the sum of Dj
    if (sum(qj_hat) != 0){
      qj_hat <- qj_hat / sum(qj_hat)} else {qj_hat <- rep(0, length(qj_hat))}
    
    
    q_hat <- qj_hat / h      # Computing the high of the histogram dividing by the width of the columns
    
    
    # Compute the function to integrate 
    to_integrate_1 <- function(x) {return(( distr(x) - p_hat_func(x,bins = bins , p_hat = p_hat ))^2)}
    to_integrate_2 <- function(x) {return(( distr(x) - q_hat_func(x,bins = bins , q_hat = q_hat ))^2)}
    
    # Compute the integral
    p <- integrate( Vectorize(to_integrate_1) , lower = 0 , upper = 1, subdivisions=2000)$value
    q <- integrate( Vectorize(to_integrate_2) , lower = 0 , upper = 1, subdivisions=2000)$value
    
    integral_p <- c(integral_p, p)
    integral_q <- c(integral_q, q)
    
  }
  mise_p <- mean(integral_p)   #Save the results
  mise_q <- mean(integral_q)   #Save the results
  return(c(mise_p , mise_q))
}
```

Here we run the simulation for the Beta model
```{r run for beta model , eval = FALSE }
##### Running Simulation  #####


m <- seq(5,7)
simulation_size <- 100

Beta_sim_n100_eps_1 <- lapply(m, simulation_function, sim_size = simulation_size, n=100 , func='beta', eps = .1)


Beta_sim_n100_eps_0001 <- lapply(m, simulation_function, sim_size = simulation_size, n=100 , func='beta', eps = 0.001)

Beta_sim_n1000_eps_1 <- lapply(m, simulation_function, sim_size = simulation_size, n=1000 , func='beta', eps = .1)


Beta_sim_n1000_eps_0001 <- lapply(m, simulation_function, sim_size = simulation_size, n=1000 , func='beta', eps = 0.001)

```


![](/Users/paolo/Desktop/Documents/GitHub/SDS_HW1/Picture/Beta_plots.png)

<div style="width:100px; height:100px">
![](/Users/paolo/Desktop/Documents/GitHub/SDS_HW1/Picture/legend.png)
</div>


### 2.2

In this task we changed the TRUE distribution Beta into a mixture od 2 Beta's.

We set the parameter $\pi = 0.6$ in order to introduce some "sparsity.

In order to compute the MISE we use the same function written before, only changing the true distribtion.

```{r  mixture distribution , eval = FALSE }

##### Mixture distribution of Beta ######
dmixture <- function(x, shape_1 = 2 , shape_2 = 15 , shape_3 = 12 , shape_4 = 6 , pi = 0.6 ){
  f <- pi * dbeta(x, shape1 = shape_1 , shape2 = shape_2) + (1 - pi) * dbeta(x, shape1 = shape_3 , shape2 = shape_4)
  return(f)
}
##### Random sample from Mixture Beta  #####
rmixture <- function(n, shape_1 = 2 , shape_2 = 15 , shape_3 = 12 , shape_4 = 6 , pi = 0.6 ){
  sam <- c()
  u <- runif(n)
  for (x in u){
    if (x < pi) sam <-c(sam, rbeta(1, shape1 = shape_1 , shape2 = shape_2))
    else sam <- c(sam , rbeta(1, shape1 = shape_3 , shape2 = shape_4) )
  }
  return(sam)
}

```

And then we run the simulations:

```{r  mixture simukations , eval = FALSE }

Mixture_sim_n100_eps_1 <- lapply(m, simulation_function, sim_size = simulation_size, n=100 , func='mixture', eps = .1)


Mixture_sim_n100_eps_0001 <- lapply(m, simulation_function, sim_size = simulation_size, n=100 , func='mixture', eps = 0.001)

Mixture_sim_n1000_eps_1 <- lapply(m, simulation_function, sim_size = simulation_size, n=1000 , func='mixture', eps = .1)


Mixture_sim_n1000_eps_0001 <- lapply(m, simulation_function, sim_size = simulation_size, n=1000 , func='mixture', eps = 0.001)

```


![](/Users/paolo/Desktop/Documents/GitHub/SDS_HW1/Picture/Mixture_plots.png)

<div style="width:100px; height:100px">
![](/Users/paolo/Desktop/Documents/GitHub/SDS_HW1/Picture/legend.png)
<div>




